{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA9m/v+FzNvoY+MOeek9FS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arkajeet7/warehouse-robotics-using-reinforcement-learning-/blob/main/agent/agent_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OSvPfFTuyZks"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import importlib.util\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WarehouseEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Warehouse Environment for RL tasks.\n",
        "\n",
        "    Grid representation:\n",
        "    - 0: walkable path\n",
        "    - 1: rack/obstacle\n",
        "    - S: start (0.5)\n",
        "    - I: intermediate (0.65)\n",
        "    - G: goal (0.75)\n",
        "    - A: agent (1.0)\n",
        "\n",
        "    Actions:\n",
        "    0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {'render_modes': ['human', 'ansi'], 'render_fps': 4}\n",
        "\n",
        "    def __init__(self, rows=11, colm=10, render_mode=None):\n",
        "        super(WarehouseEnv, self).__init__()\n",
        "\n",
        "        self.rows = rows\n",
        "        self.colm = colm\n",
        "        self.render_mode = render_mode\n",
        "        self.reached_intermediate = False\n",
        "        self.episode_num = 0\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(rows, colm), dtype=np.float32)\n",
        "\n",
        "        self.warehouse_layout = self._create_default_layout()\n",
        "\n",
        "        self.start_pos = self.random_choice()\n",
        "\n",
        "        #checks if the intermediate and the goal are equal\n",
        "        while True:\n",
        "          self.intermediate_pos = self.random_choice()\n",
        "          SI = self.manhattan(self.start_pos, self.intermediate_pos)\n",
        "          if self.intermediate_pos!=self.start_pos and SI>3:\n",
        "            break\n",
        "        #checks if the goal position is not equal to start and intermediate\n",
        "        while True:\n",
        "          self.goal_pos = self.random_choice()\n",
        "          IG = self.manhattan(self.intermediate_pos, self.goal_pos)\n",
        "          if self.goal_pos!=self.intermediate_pos and IG>3 :\n",
        "            break\n",
        "        self.steps = 0\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.max_steps = rows * colm * 32\n",
        "\n",
        "    def random_choice(self):\n",
        "      while True:\n",
        "        r=random.randint(0,self.rows-1)\n",
        "        c=random.randint(0,self.colm-1)\n",
        "        if self.warehouse_layout[r,c]==0:\n",
        "          return r,c\n",
        "\n",
        "    def manhattan(self,a,b):\n",
        "      return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
        "\n",
        "\n",
        "    def calculate_distance_from_goal(self, position):\n",
        "        goal_x, goal_y = self.goal_pos\n",
        "        pos_x, pos_y = position\n",
        "        distance = self. manhattan((goal_x, goal_y), (pos_x, pos_y))\n",
        "        return distance\n",
        "\n",
        "    def calculate_distance_from_intermediate(self, position):\n",
        "        intermediate_x, intermediate_y = self.intermediate_pos\n",
        "        pos_x, pos_y = position\n",
        "        distance = self.manhattan((intermediate_x, intermediate_y), (pos_x, pos_y))\n",
        "        return distance\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # ---- RANDOMIZE EVERY NEW EPISODE ----\n",
        "        self.start_pos = (2,4)\n",
        "        self.intermediate_pos=(4,4)\n",
        "        self.goal_pos=(6,4)\n",
        "        \"\"\"\n",
        "        while True:\n",
        "          self.intermediate_pos = self.random_choice()\n",
        "          SI = self.manhattan(self.start_pos, self.intermediate_pos)\n",
        "          if self.intermediate_pos != self.start_pos and SI>3:\n",
        "            break\n",
        "\n",
        "        while True:\n",
        "          self.goal_pos = self.random_choice()\n",
        "          IG = self.manhattan(self.intermediate_pos, self.goal_pos)\n",
        "          if self.goal_pos != self.intermediate_pos and IG>3:\n",
        "            break\"\"\"\n",
        "\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.steps = 0\n",
        "        self.reached_intermediate = False\n",
        "        self.neg_step_counter=0\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def _get_observation(self):\n",
        "        obs = self.warehouse_layout.copy().astype(np.float32)\n",
        "        r, c = self.agent_pos\n",
        "        sr, sc = self.start_pos\n",
        "        ir, ic = self.intermediate_pos\n",
        "        gr, gc = self.goal_pos\n",
        "\n",
        "        obs[sr, sc] = 0.5\n",
        "        obs[ir, ic] = 0.65\n",
        "        obs[gr, gc] = 0.75\n",
        "        obs[r, c] = -1.0\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "      self.steps += 1\n",
        "      prev_row, prev_col = self.agent_pos\n",
        "      row, col = prev_row, prev_col\n",
        "      reward = 0\n",
        "      terminated=False\n",
        "\n",
        "      # ============================================\n",
        "      # >>>REWARD-1 : distance between goals reward\n",
        "      # ============================================\n",
        "      \"\"\"\n",
        "      IG = self.manhattan(self.intermediate_pos, self.goal_pos)\n",
        "      SI = self.manhattan(self.start_pos, self.intermediate_pos)\n",
        "      reward +=0.02*(IG+SI)\"\"\"\n",
        "\n",
        "\n",
        "      # --- Movement logic ---\n",
        "      if action == 0:   # LEFT\n",
        "        col = max(0, col - 1)\n",
        "      elif action == 1: # DOWN\n",
        "        row = min(self.rows - 1, row + 1)\n",
        "      elif action == 2: # RIGHT\n",
        "        col = min(self.colm - 1, col + 1)\n",
        "      elif action == 3: # UP\n",
        "        row = max(0, row - 1)\n",
        "\n",
        "      # --- Check movement validity ---\n",
        "      # ðŸ§± Hit obstacle\n",
        "      if 0 <= row < self.rows and 0 <= col < self.colm:\n",
        "        # =======================================\n",
        "        # >>>REWARD-2: step penalty\n",
        "        # ========================================\n",
        "\n",
        "        reward -= 0.01\n",
        "\n",
        "        if self.warehouse_layout[row, col] == 1:\n",
        "            row, col = prev_row, prev_col\n",
        "            reward -= 5.0\n",
        "            terminated = False\n",
        "\n",
        "        else:\n",
        "          #checking for intermediate reached\n",
        "          if self.reached_intermediate==True:\n",
        "            if (row,col)==self.goal_pos:\n",
        "              # ==========================================\n",
        "              # >>>REWARD-3: reward for reaching the goal\n",
        "              # ==========================================\n",
        "\n",
        "              reward =50\n",
        "              terminated=True\n",
        "\n",
        "            else:\n",
        "              # ========================================================\n",
        "              # >>>REWARD-4: distance between agent pos and goal reward\n",
        "              # ========================================================\n",
        "\n",
        "              prev_dist_1=self.calculate_distance_from_goal((prev_row,prev_col))\n",
        "              new_dist_1=self.calculate_distance_from_goal((row,col))\n",
        "              diff_1=prev_dist_1 - new_dist_1\n",
        "              reward +=1*diff_1\n",
        "\n",
        "           #checking for intermediate not reached\n",
        "\n",
        "          if self.reached_intermediate==False:\n",
        "            if (row,col)==self.intermediate_pos:\n",
        "\n",
        "              # ============================================\n",
        "              # >>>REWARD-5: for reaching intermediate goal\n",
        "              # ============================================\n",
        "              reward = 3\n",
        "              terminated=False\n",
        "              self.reached_intermediate = True\n",
        "\n",
        "            else:\n",
        "              # ================================================================\n",
        "              # >>>REWARD-6: distance reward between agent pos and intermediate\n",
        "              # ================================================================\n",
        "              prev_dist_2=self.calculate_distance_from_intermediate((prev_row,prev_col))\n",
        "              new_dist_2=self.calculate_distance_from_intermediate((row,col))\n",
        "              diff_2= prev_dist_2 - new_dist_2\n",
        "              reward += 0.1*diff_2\n",
        "              terminated=False\n",
        "\n",
        "            # =======================================================\n",
        "            # >>>REWARD-7: damping reward (for escaping local maxima)\n",
        "            # =======================================================\n",
        "            \"\"\"\n",
        "          if reward<0:\n",
        "             # increase counter for consecutive negative-movement steps\n",
        "             self.neg_step_counter += 1\n",
        "\n",
        "             # cap the counter at 6 (after 6 steps boost becomes 0)\n",
        "             step_count = min(self.neg_step_counter, 6)\n",
        "\n",
        "             # linear decay: goes from A â†’ 0 in exactly 6 steps\n",
        "             A = 4  # maximum boost value, tuneable\n",
        "             add_value = A * (1 - (step_count - 1) / 5)\n",
        "\n",
        "             if add_value < 0:\n",
        "              add_value = 0\n",
        "\n",
        "             # apply correction\n",
        "             reward = reward + add_value\n",
        "          else:\n",
        "            # reset counter because reward is not negative\n",
        "            self.neg_step_counter = 0\"\"\"\n",
        "\n",
        "      else:\n",
        "        # ðŸš« Out of bounds\n",
        "        # ======================================\n",
        "        # >>>REWARD-8: out of boundary penalty\n",
        "        # ======================================\n",
        "        row, col = prev_row, prev_col\n",
        "        reward = -3.0\n",
        "        terminated = False\n",
        "\n",
        "    # --- Truncation check ---\n",
        "    # ================================\n",
        "    # >>>REWARD-9: truncation penalty\n",
        "    # ================================\n",
        "      truncated = self.steps >= self.max_steps\n",
        "      if truncated:\n",
        "        reward -= 30.0\n",
        "\n",
        "    # ===================================\n",
        "    # >>>REWARD-10: clipping\n",
        "    # ===================================\n",
        "\n",
        "\n",
        "    # --- Update agent position ---\n",
        "      self.agent_pos = (row, col)\n",
        "      observation = self._get_observation()\n",
        "      info = {}\n",
        "\n",
        "      return observation, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"ansi\":\n",
        "            return self._render_text()\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def _render_text(self):\n",
        "        grid = self.warehouse_layout.copy()\n",
        "        r, c = self.agent_pos\n",
        "        output = \"\"\n",
        "        for i in range(self.rows):\n",
        "            for j in range(self.colm):\n",
        "                if (i, j) == self.agent_pos:\n",
        "                    output += \"A \"\n",
        "                elif (i, j) == self.goal_pos:\n",
        "                    output += \"G \"\n",
        "                elif (i, j) == self.intermediate_pos:\n",
        "                    output += \"I \"\n",
        "                elif (i, j) == self.start_pos:\n",
        "                    output += \"S \"\n",
        "                else:\n",
        "                    output += \"â–ˆ \" if grid[i, j] == 1 else \". \"\n",
        "            output += \"\\n\"\n",
        "        return output\n",
        "\n",
        "    def close(self):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "T0o2WXgW3BHo"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}