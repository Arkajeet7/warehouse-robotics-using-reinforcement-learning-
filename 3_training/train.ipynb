{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQt8EA/MqZu9YwRcVBGK1B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arkajeet7/warehouse-robotics-using-reinforcement-learning-/blob/main/training/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSvPfFTuyZks"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import importlib.util\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WarehouseDQL():\n",
        "    # Hyperparameters\n",
        "    learning_rate_a = 0.0001\n",
        "    discount_factor_g = 0.99\n",
        "    network_sync_rate = 1000\n",
        "    replay_memory_size = 100000\n",
        "    mini_batch_size = 64\n",
        "\n",
        "    # Neural Network\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "    optimizer = None\n",
        "\n",
        "    ACTIONS = ['L', 'D', 'R', 'U']\n",
        "\n",
        "    def train(self, episodes, render=False):\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Create Warehouse environment\n",
        "        env = WarehouseEnv(render_mode='human' if render else None)\n",
        "        rows, cols = env.rows, env.colm\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        epsilon = 1  # Start with 100% random actions\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target networks\n",
        "        policy_dqn = DQN(rows=rows, cols=cols, h1_nodes=256, out_actions=num_actions).to(device)\n",
        "        target_dqn = DQN(rows=rows, cols=cols, h1_nodes=256, out_actions=num_actions).to(device)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # Training metrics\n",
        "        rewards_per_episode = []\n",
        "        epsilon_history = []\n",
        "        step_count_log=[]\n",
        "        train_steps = 0\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state, _ = env.reset()\n",
        "            episode_positions = [env.agent_pos]\n",
        "            episode_reward = 0\n",
        "            step_count = 0\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "\n",
        "            while not (terminated or truncated):\n",
        "                # --------------------------\n",
        "                # 1. conversion of state to tensor\n",
        "                # --------------------------\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "                # -----------------------------------------------\n",
        "                # 2. exploration based on epsilon greedy function\n",
        "                # -----------------------------------------------\n",
        "                if random.random() < epsilon:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                       policy_dqn.eval()\n",
        "                       action = policy_dqn(state_tensor).argmax().item()\n",
        "                    policy_dqn.train()\n",
        "                # ------------------------------------\n",
        "                # 3. step taken based on warehouse env\n",
        "                # ------------------------------------\n",
        "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "                episode_positions.append(env.agent_pos)\n",
        "\n",
        "                # ---------------------------\n",
        "                # 4. memory save\n",
        "                # ---------------------------\n",
        "                reward=float(np.clip(reward, -10, 10))\n",
        "                memory.append((state, action, new_state, reward, terminated or truncated))\n",
        "\n",
        "                # ----------------------------\n",
        "                # 5. next state update\n",
        "                # ----------------------------\n",
        "                state = new_state\n",
        "                step_count += 1\n",
        "                step_count_log.append(step_count)\n",
        "\n",
        "                # -------------------------\n",
        "                # 6. training on mini batch\n",
        "                # -------------------------\n",
        "                if len(memory) > self.mini_batch_size:\n",
        "                    mini_batch = memory.sample(self.mini_batch_size)\n",
        "                    self.optimize(mini_batch, policy_dqn, target_dqn, rows, cols,device)\n",
        "                    train_steps += 1\n",
        "\n",
        "                   # ---------------------------------\n",
        "                   # 7. syncing the target and policy\n",
        "                   # ---------------------------------\n",
        "                    if train_steps % self.network_sync_rate == 0:\n",
        "                      target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "            # -------------------------------------\n",
        "            # 8. appending the position and reward\n",
        "            # -------------------------------------\n",
        "            rewards_per_episode.append(episode_reward)\n",
        "            all_episode_position.append(episode_positions)\n",
        "\n",
        "            # --------------------\n",
        "            # 9. epsilon decay\n",
        "            # --------------------\n",
        "            epsilon = max(0.05, epsilon * 0.9995)  # Don't go below 1% random actions\n",
        "            epsilon_history.append(epsilon)\n",
        "\n",
        "            # ---------------------------------\n",
        "            # 10. syncing the target and policy\n",
        "            # ---------------------------------\n",
        "\n",
        "\n",
        "            # -----------------------------\n",
        "            # 11. result printing\n",
        "            # -----------------------------\n",
        "            print(f\"\\nEpisode {i+1}/{episodes}\")\n",
        "            print(f\"  final Reward: {reward}\")\n",
        "            print(f\"episodic reward: {episode_reward}\")\n",
        "            print(f\"  Epsilon: {epsilon:.3f}\")\n",
        "            print(f\"termination status={terminated}\")\n",
        "            print(f\"truncation status={truncated}\")\n",
        "            print(f\"steps taken={step_count}\")\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "\n",
        "        # -----------------\n",
        "        # 11. save model\n",
        "        # -----------------\n",
        "        torch.save(policy_dqn.state_dict(), \"warehouse_dql.pt\")\n",
        "\n",
        "        # ------------------------------------\n",
        "        # 12. plotting total reward vs episode\n",
        "        # ------------------------------------\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(121)\n",
        "        plt.plot(rewards_per_episode)\n",
        "        plt.title('Rewards per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.plot(epsilon_history)\n",
        "        plt.title('Epsilon Decay')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Epsilon')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('warehouse_training.png')\n",
        "\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn, rows, cols,device):\n",
        "        policy_dqn.train()\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        next_states = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "\n",
        "        # Separate batch into individual arrays\n",
        "        for state, action, next_state, reward, done in mini_batch:\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            next_states.append(next_state)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(np.array(states)).to(device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
        "        dones = torch.BoolTensor(dones).unsqueeze(1).to(device)\n",
        "\n",
        "        # --------------------------\n",
        "        # 1. compute current q value\n",
        "        # --------------------------\n",
        "\n",
        "        current_q =  policy_dqn(states).gather(1, actions)\n",
        "\n",
        "        # ------------------------------\n",
        "        # 2. compute the next q-values\n",
        "        # ------------------------------\n",
        "        with torch.no_grad():\n",
        "\n",
        "          next_actions = policy_dqn(next_states).argmax(dim=1, keepdim=True)\n",
        "\n",
        "          next_q = target_dqn(next_states).gather(1, next_actions)\n",
        "\n",
        "          next_q[dones] = 0.0\n",
        "\n",
        "        # ----------------------------------\n",
        "        # 3. Compute TD target\n",
        "        # ----------------------------------\n",
        "        gamma = self.discount_factor_g\n",
        "        target_q = rewards + gamma * next_q.detach()\n",
        "\n",
        "        # ----------------------------------\n",
        "        # 4. loss and backprop\n",
        "        # ----------------------------------\n",
        "\n",
        "        loss = self.loss_fn(current_q, target_q)\n",
        "\n",
        "        if not loss.requires_grad:\n",
        "         return None\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(policy_dqn.parameters(), max_norm=10.0)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def test(self, episodes):\n",
        "        # Create environment\n",
        "        env = WarehouseEnv(render_mode='human')\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        rows, cols = env.rows, env.colm\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        # Load trained policy\n",
        "        policy_dqn = DQN(rows=rows, cols=cols, h1_nodes=256, out_actions=num_actions).to(device)\n",
        "        ckpt = torch.load(\"warehouse_dql.pt\", map_location=device)\n",
        "        policy_dqn.load_state_dict(ckpt)\n",
        "        policy_dqn.eval()\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state, _ = env.reset()\n",
        "            total_reward = 0\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            test_agent_pos=[env.agent_pos]\n",
        "            while not (terminated or truncated):\n",
        "                # Select best action\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                    action = int(policy_dqn(state_tensor).argmax().item())\n",
        "\n",
        "\n",
        "                # Execute action\n",
        "                state, reward, terminated, truncated, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                test_agent_pos.append(env.agent_pos)\n",
        "\n",
        "                #printing the path taken\n",
        "\n",
        "\n",
        "            print(f\"Episode {i+1}: Total Reward = {total_reward}\")\n",
        "            self.plot_path(env, test_agent_pos)\n",
        "\n",
        "    def plot_path(self, env, path):\n",
        "      layout = env.warehouse_layout.copy()\n",
        "\n",
        "      # Create a figure\n",
        "      plt.figure(figsize=(8, 6))\n",
        "      plt.imshow(layout, cmap='Greys', origin='upper')\n",
        "\n",
        "      # Unpack path coordinates\n",
        "      rows = [pos[0] for pos in path]\n",
        "      cols = [pos[1] for pos in path]\n",
        "\n",
        "      # Plot path\n",
        "      plt.plot(cols, rows, color='blue', linewidth=2, marker='o',alpha=0.7, markersize=4, label='Agent Path')\n",
        "\n",
        "      # Mark start and goal\n",
        "      start_r, start_c = env.start_pos\n",
        "      goal_r, goal_c = env.goal_pos\n",
        "      intermediate_r,intermediate_c=env.intermediate_pos\n",
        "      plt.scatter(start_c, start_r, color='green', s=300, label='Start', marker='s',zorder=5)\n",
        "      plt.scatter(goal_c, goal_r, color='red', s=300, label='Goal', marker='*',zorder=5)\n",
        "      plt.scatter(intermediate_c,intermediate_r,color='yellow',label='intermediate',marker='*',zorder=5)\n",
        "\n",
        "    # Customize grid\n",
        "      plt.title(\"Agent Path in Warehouse\")\n",
        "      plt.legend(loc='lower right')\n",
        "      plt.gca().invert_yaxis()  # to align (row, col) with visual top-down grid\n",
        "      plt.xticks(range(env.colm))\n",
        "      plt.yticks(range(env.rows))\n",
        "      plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "      plt.savefig('path for 1 agent with intermediate, variable goal and intermediate ')\n",
        "      plt.show()\n",
        "\n",
        "      env.close()\n"
      ],
      "metadata": {
        "id": "MdrJTlJdH-Ov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
